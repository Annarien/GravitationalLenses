{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KerasCnn_20201014.py on full dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from astropy.utils.data import get_pkg_data_filename\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from tensorflow.python.keras.layers.core import Dense, Dropout, Flatten\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "\n",
    "from ExcelUtils import createExcelSheet, writeToFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14_10_2020_21_42_34\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "print(dt_string)\n",
    "excel_headers = []\n",
    "excel_dictionary = []\n",
    "excel_headers.append(\"Date and Time\")\n",
    "excel_dictionary.append(dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "makeNewCSVFile = True\n",
    "max_num = sys.maxsize  # Set to sys.maxsize when running entire data set\n",
    "max_num_testing = sys.maxsize  # Set to sys.maxsize when running entire data set\n",
    "max_num_prediction = sys.maxsize  # Set to sys.maxsize when running entire data set\n",
    "validation_split = 0.2  # A float value between 0 and 1 that determines what percentage of the training\n",
    "# data is used for validation.\n",
    "k_fold_num = 2  # A number between 1 and 10 that determines how many times the k-fold classifier\n",
    "# is trained.\n",
    "epochs = 3  # A number that dictates how many iterations should be run to train the classifier\n",
    "batch_size = 128  # The number of items batched together during training.\n",
    "run_k_fold_validation = True  # Set this to True if you want to run K-Fold validation as well.\n",
    "input_shape = (100, 100, 3)  # The shape of the images being learned & evaluated.\n",
    "augmented_multiple = 2  # This uses data augmentation to generate x-many times as much data as there is on file.\n",
    "use_augmented_data = True  # Determines whether to use data augmentation or not.\n",
    "patience_num = 10  # Used in the early stopping to determine how quick/slow to react.\n",
    "use_early_stopping = True  # Determines whether to use early stopping or not.\n",
    "use_model_checkpoint = True  # Determines whether the classifiers keeps track of the most accurate iteration of itself.\n",
    "monitor_early_stopping = 'val_loss'\n",
    "monitor_model_checkpoint = 'val_acc'\n",
    "use_shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding global parameters to excel\n",
    "excel_headers.append(\"Max Training Num\")\n",
    "excel_dictionary.append(max_num)\n",
    "excel_headers.append(\"Max Testing Num\")\n",
    "excel_dictionary.append(max_num_testing)\n",
    "excel_headers.append(\"Max Prediction Num\")\n",
    "excel_dictionary.append(max_num_prediction)\n",
    "excel_headers.append(\"Validation Split\")\n",
    "excel_dictionary.append(validation_split)\n",
    "excel_headers.append(\"K fold Num\")\n",
    "excel_dictionary.append(k_fold_num)\n",
    "excel_headers.append(\"Epochs\")\n",
    "excel_dictionary.append(epochs)\n",
    "excel_headers.append(\"Batch Size\")\n",
    "excel_dictionary.append(batch_size)\n",
    "excel_headers.append(\"Run K fold\")\n",
    "excel_dictionary.append(run_k_fold_validation)\n",
    "excel_headers.append(\"Input Shape\")\n",
    "excel_dictionary.append(input_shape)\n",
    "excel_headers.append(\"Augmented Multiple\")\n",
    "excel_dictionary.append(augmented_multiple)\n",
    "excel_headers.append(\"Use Augmented Data\")\n",
    "excel_dictionary.append(use_augmented_data)\n",
    "excel_headers.append(\"Patience\")\n",
    "excel_dictionary.append(patience_num)\n",
    "excel_headers.append(\"Use Early Stopping\")\n",
    "excel_dictionary.append(use_early_stopping)\n",
    "excel_headers.append(\"Use Model Checkpoint\")\n",
    "excel_dictionary.append(use_model_checkpoint)\n",
    "excel_headers.append(\"Monitor Early Stopping\")\n",
    "excel_dictionary.append(monitor_early_stopping)\n",
    "excel_headers.append(\"Monitor Model Checkpoint\")\n",
    "excel_dictionary.append(monitor_model_checkpoint)\n",
    "\n",
    "if not os.path.exists('../Results/%s/' % dt_string):\n",
    "    os.mkdir('../Results/%s/' % dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "def getPositiveImages(images_dir, max_num, input_shape):\n",
    "    \"\"\"\n",
    "    This gets the positively simulated images in the g, r and  i bands.\n",
    "    Args:\n",
    "        images_dir(string): This is the file path address of the positively simulated images.\n",
    "        max_num(integer):   This is the number of sources of the positively simulated images to be used.\n",
    "        input_shape(tuple): This is the shape of the images.\n",
    "    Returns:\n",
    "        positive_images(numpy array):   This is the numpy array of the positively simulated images with the shape of\n",
    "                                        (num of images, input_shape[0], input_shape[1], input_shape[2]) =\n",
    "                                        (num_of_images, 100, 100, 3).\n",
    "    \"\"\"\n",
    "    for root, dirs, _ in os.walk(images_dir):\n",
    "        num_of_images = min(max_num, len(dirs))\n",
    "        positive_images = np.zeros([num_of_images, 3, 100, 100])\n",
    "        index = 0\n",
    "        for folder in dirs:\n",
    "            g_img_path = get_pkg_data_filename('%s/%s_g_norm.fits' % (os.path.join(root, folder), folder))\n",
    "            r_img_path = get_pkg_data_filename('%s/%s_r_norm.fits' % (os.path.join(root, folder), folder))\n",
    "            i_img_path = get_pkg_data_filename('%s/%s_i_norm.fits' % (os.path.join(root, folder), folder))\n",
    "\n",
    "            g_data = fits.open(g_img_path)[0].data[0:100, 0:100]\n",
    "            r_data = fits.open(r_img_path)[0].data[0:100, 0:100]\n",
    "            i_data = fits.open(i_img_path)[0].data[0:100, 0:100]\n",
    "\n",
    "            img_data = [g_data, r_data, i_data]\n",
    "            positive_images[index] = img_data\n",
    "            index += 1\n",
    "\n",
    "            if index >= num_of_images:\n",
    "                break\n",
    "        return positive_images.reshape(num_of_images, input_shape[0], input_shape[1], input_shape[2])\n",
    "\n",
    "\n",
    "def getNegativeImages(images_dir, max_num, input_shape):\n",
    "    \"\"\"\n",
    "    This gets the negative images in the g, r and  i bands.\n",
    "    Args:\n",
    "        images_dir(string): This is the file path address of the negative images.\n",
    "        max_num(integer):   This is the number of sources of the negative images to be used.\n",
    "        input_shape(tuple): This is the shape of the images.\n",
    "    Returns:\n",
    "        negative_images(numpy array):   This is the numpy array of the negative images with the shape of\n",
    "                                        (num of images, input_shape[0], input_shape[1], input_shape[2]) =\n",
    "                                        (num_of_images, 100, 100, 3).\n",
    "    \"\"\"\n",
    "    for root, dirs, _ in os.walk(images_dir):\n",
    "        num_of_images = min(max_num, len(dirs))\n",
    "        negative_images = np.zeros([num_of_images, 3, 100, 100])\n",
    "        index = 0\n",
    "        for folder in dirs:\n",
    "            g_img_path = get_pkg_data_filename('%s/g_norm.fits' % (os.path.join(root, folder)))\n",
    "            r_img_path = get_pkg_data_filename('%s/r_norm.fits' % (os.path.join(root, folder)))\n",
    "            i_img_path = get_pkg_data_filename('%s/i_norm.fits' % (os.path.join(root, folder)))\n",
    "\n",
    "            g_data = fits.open(g_img_path)[0].data[0:100, 0:100]\n",
    "            r_data = fits.open(r_img_path)[0].data[0:100, 0:100]\n",
    "            i_data = fits.open(i_img_path)[0].data[0:100, 0:100]\n",
    "\n",
    "            img_data = [g_data, r_data, i_data]\n",
    "            negative_images[index] = img_data\n",
    "            index += 1\n",
    "\n",
    "            if index >= num_of_images:\n",
    "                break\n",
    "        return negative_images.reshape(num_of_images, input_shape[0], input_shape[1], input_shape[2])\n",
    "\n",
    "\n",
    "def getUnseenData(images_dir, max_num, input_shape):\n",
    "    \"\"\"\n",
    "        This gets the unseen images in the g, r and  i bands containing the identified known lenses.\n",
    "        Args:\n",
    "            images_dir(string): This is the file path address of the unseen images.\n",
    "            max_num(integer):   This is the number of sources of the unseen images to be used.\n",
    "            input_shape(tuple): This is the shape of the images.\n",
    "        Returns:\n",
    "            des_tiles(dictionary):   This is the dictionary of the unseen images with the shape of\n",
    "                                            (num of images, input_shape[0], input_shape[1], input_shape[2]) =\n",
    "                                            (num_of_images, 100, 100, 3).\n",
    "        \"\"\"\n",
    "\n",
    "    des_tiles = {}\n",
    "\n",
    "    for root, dirs, _ in os.walk(images_dir):\n",
    "        num_of_images = min(max_num, len(dirs))\n",
    "        index = 0\n",
    "        for folder in dirs:\n",
    "            g_img_path = get_pkg_data_filename('%s/g_norm.fits' % (os.path.join(root, folder)))\n",
    "            r_img_path = get_pkg_data_filename('%s/r_norm.fits' % (os.path.join(root, folder)))\n",
    "            i_img_path = get_pkg_data_filename('%s/i_norm.fits' % (os.path.join(root, folder)))\n",
    "\n",
    "            g_data = fits.open(g_img_path)[0].data[0:100, 0:100]\n",
    "            r_data = fits.open(r_img_path)[0].data[0:100, 0:100]\n",
    "            i_data = fits.open(i_img_path)[0].data[0:100, 0:100]\n",
    "\n",
    "            img_data = np.array([g_data, r_data, i_data]).reshape(input_shape[0], input_shape[1], input_shape[2])\n",
    "            des_tiles.update({folder: img_data})\n",
    "            index += 1\n",
    "            if index >= num_of_images:\n",
    "                break\n",
    "\n",
    "        return des_tiles\n",
    "\n",
    "\n",
    "def makeImageSet(positive_images, negative_images=None, known_des_names=None, neg_des_names=None,\n",
    "                 shuffle_needed=use_shuffle):\n",
    "    \"\"\"\n",
    "    This is used to create data set of images and labels, in which the positive and negative images are all\n",
    "    combined and shuffled.\n",
    "    Args:\n",
    "        positive_images(numpy array):   This is the numpy array of the positively simulated images.\n",
    "        negative_images(numpy array):   This is the numpy array of the negative images, this is set to a\n",
    "                                        default of None.\n",
    "        known_des_names(list):    This is the dictionary of the unseen known lenses, this is set to a\n",
    "                                        default of None.\n",
    "        neg_des_names(list):      This is the dictionary of the negative images, this is set to a\n",
    "                                        default of None.\n",
    "        shuffle_needed(boolean):        This is a boolean value to determine whether or not shuffling of the given data\n",
    "                                        sets is required.\n",
    "    Returns:\n",
    "        image_set(numpy array):         This is the image data set of  numpy array of the combination positive\n",
    "                                        and negative images.\n",
    "        label_set(numpy array):         This is the label data set of  numpy array of the combination positive\n",
    "                                        and negative label.\n",
    "        des_names_set(numpy array):     This is the des name data set of the known lenses and negative images used.\n",
    "    \"\"\"\n",
    "    if negative_images is None:\n",
    "        negative_images = []\n",
    "        known_des_names = {}\n",
    "        neg_des_names = {}\n",
    "\n",
    "    image_set = []\n",
    "    label_set = []\n",
    "    des_names_set = []\n",
    "\n",
    "    # If there is none in objects for the known_des_names and neg_des_names\n",
    "    if known_des_names is None and neg_des_names is None:\n",
    "        for index_none in range(0, len(positive_images)):\n",
    "            image_set.append(positive_images[index_none])\n",
    "            label_set.append(1)\n",
    "\n",
    "        for index_none in range(0, len(negative_images)):\n",
    "            image_set.append(negative_images[index_none])\n",
    "            label_set.append(0)\n",
    "\n",
    "        if shuffle_needed:\n",
    "            image_set, label_set = shuffle(image_set, label_set)\n",
    "\n",
    "    else:  # if there is names for des\n",
    "        for index_des in range(0, len(positive_images)):\n",
    "            image_set.append(positive_images[index_des])\n",
    "            label_set.append(1)\n",
    "            des_names_set.append(known_des_names[index_des])\n",
    "\n",
    "        for index_des in range(0, len(negative_images)):\n",
    "            image_set.append(negative_images[index_des])\n",
    "            label_set.append(0)\n",
    "            des_names_set.append(neg_des_names[index_des])\n",
    "\n",
    "        if shuffle_needed:\n",
    "            image_set, label_set, des_names_set = shuffle(image_set, label_set, des_names_set)\n",
    "\n",
    "    return np.array(image_set), np.array(label_set), np.array(des_names_set)\n",
    "\n",
    "\n",
    "def buildClassifier(input_shape=(100, 100, 3)):\n",
    "    \"\"\"\n",
    "    This creates the CNN algorithm.\n",
    "    Args:\n",
    "        input_shape(tuple): This is the image shape of (100,100,3)\n",
    "    Returns:\n",
    "        classifier(sequential): This is the sequential model.\n",
    "    \"\"\"\n",
    "    # Initialising the CNN\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "    classifier.add(Dropout(0.5))  # added extra Dropout layer\n",
    "    classifier.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    classifier.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    classifier.add(Dropout(0.5))  # added extra dropout layer\n",
    "    classifier.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    classifier.add(Dropout(0.2))  # antes era 0.25\n",
    "    classifier.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "    classifier.add(Conv2D(1024, (3, 3), activation='relu', padding='same'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    classifier.add(Dense(units=1024, activation='relu'))  # added new dense layer\n",
    "    classifier.add(Dropout(0.2))  # antes era 0.25\n",
    "    # Step 3 - Flattening\n",
    "    classifier.add(Flatten())\n",
    "    classifier.add(Dense(units=1024, activation='relu'))  # added new dense layer\n",
    "    classifier.add(Dense(units=256, activation='relu'))  # added new dense layer\n",
    "    # Step 4 - Full connection\n",
    "    classifier.add(Dropout(0.2))\n",
    "    classifier.add(Dense(units=1, activation='sigmoid'))\n",
    "    classifier.summary()\n",
    "\n",
    "    # Compiling the CNN\n",
    "    classifier.compile(optimizer='adam',\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "    plot_model(classifier, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    return classifier\n",
    "\n",
    "\n",
    "def visualiseActivations(img_tensor, base_dir):\n",
    "    \"\"\"\n",
    "    This makes images of the activations, as the selected image passed through the model\n",
    "    Args:\n",
    "        img_tensor(numpy array):    This is the numpy array of the selected image\n",
    "        base_dir(string):           This is the file path name\n",
    "    Saves:\n",
    "        This saves the activation images of the selected source.\n",
    "    \"\"\"\n",
    "    global predicted_class, size\n",
    "    # Run prediction on that image\n",
    "    predicted_class = classifier.predict_classes(img_tensor, batch_size=10)\n",
    "    print(\"Predicted class is: \", predicted_class)\n",
    "    # Visualize activations\n",
    "    layer_outputs = [layer.output for layer in classifier.layers[:12]]\n",
    "    activation_model = Model(inputs=classifier.input, outputs=layer_outputs)\n",
    "    activations = activation_model.predict(img_tensor)\n",
    "    layer_names = []\n",
    "    for layer in classifier.layers[:12]:\n",
    "        layer_names.append(layer.name)\n",
    "    images_per_row = 3\n",
    "    count = 0\n",
    "    for layer_name, layer_activation in zip(layer_names, activations):\n",
    "        number_of_features = layer_activation.shape[-1]\n",
    "        size = layer_activation.shape[1]\n",
    "        number_of_columns = number_of_features // images_per_row\n",
    "        display_grid = np.zeros((size * number_of_columns, images_per_row * size))\n",
    "        for col in range(number_of_columns):\n",
    "            for row in range(images_per_row):\n",
    "                channel_image = layer_activation[0, :, :, col * images_per_row + row]\n",
    "                channel_image -= channel_image.mean()\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "                display_grid[col * size: (col + 1) * size, row * size: (row + 1) * size] = channel_image\n",
    "        scale = 1. / size\n",
    "        activations_figure = plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                                                 scale * display_grid.shape[0]))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "        activations_figure.savefig('%s/%s_Activation_%s.png' % (base_dir, count, layer_name))\n",
    "        plt.close()\n",
    "\n",
    "        count += 1\n",
    "\n",
    "\n",
    "def usingCnnModel(training_data, training_labels, val_data, val_labels):\n",
    "    \"\"\"\n",
    "    This is using the CNN model and setting it up.\n",
    "    Args:\n",
    "        training_data(numpy arrays):    This is the numpy array of the training data.\n",
    "        training_labels(numpy arrays):  This is the numpy array of the training labels.\n",
    "        val_data(numpy arrays):         This is the numpy array of the validation data.\n",
    "        val_labels(numpy arrays):       This is the numpy array of the validation labels.\n",
    "    Returns:\n",
    "        history(history):               This is the history of the classifier.\n",
    "        classifier(sequential):         This is the cnn model classifier fitted to the training data and labels.\n",
    "    \"\"\"\n",
    "    model_checkpoint = ModelCheckpoint(filepath=\"best_weights.hdf5\",\n",
    "                                       monitor=monitor_model_checkpoint,\n",
    "                                       save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor=monitor_early_stopping, patience=patience_num)  # original patience =3\n",
    "\n",
    "    classifier = buildClassifier()\n",
    "    callbacks_array = []\n",
    "    if use_early_stopping:\n",
    "        callbacks_array.append(early_stopping)\n",
    "    if use_model_checkpoint:\n",
    "        callbacks_array.append(model_checkpoint)\n",
    "\n",
    "    print(len(training_data))\n",
    "    history = classifier.fit(training_data,\n",
    "                             training_labels,\n",
    "                             epochs=epochs,\n",
    "                             validation_data=(val_data, val_labels),\n",
    "                             callbacks=callbacks_array,\n",
    "                             batch_size=batch_size\n",
    "                             # steps_per_epoch=int(len(training_data) / batch_size),\n",
    "                             )\n",
    "    return history, classifier\n",
    "\n",
    "\n",
    "def createAugmentedData(training_data, training_labels):\n",
    "    \"\"\"\n",
    "    This is creates the augmented data.\n",
    "    Args:\n",
    "        training_data(numpy arrays):    This is the numpy array of the training data.\n",
    "        training_labels(numpy arrays):  This is the numpy array of the training labels.\n",
    "    Returns:\n",
    "        complete_training_data_set(numpy array):    This is the numpy array of the total training data, which is has\n",
    "                                                    undergone augmentation.\n",
    "        complete_training_labels_set(numpy array):  This is the numpy array of the total training labels, which is has\n",
    "                                                    undergone augmentation.\n",
    "    \"\"\"\n",
    "    complete_training_data_set = []\n",
    "    complete_training_labels_set = []\n",
    "\n",
    "    for data in training_data:\n",
    "        complete_training_data_set.append(data)\n",
    "    print(\"Complete Training Data: \" + str(len(complete_training_data_set)))\n",
    "\n",
    "    for label in training_labels:\n",
    "        complete_training_labels_set.append(label)\n",
    "    print(\"Complete Training Label: \" + str(len(complete_training_labels_set)))\n",
    "\n",
    "    # create augmented data\n",
    "    data_augmented = ImageDataGenerator(featurewise_center=True,\n",
    "                                        featurewise_std_normalization=True,\n",
    "                                        rotation_range=90,\n",
    "                                        width_shift_range=0.2,\n",
    "                                        height_shift_range=0.2,\n",
    "                                        horizontal_flip=True,\n",
    "                                        vertical_flip=True)\n",
    "    data_augmented.fit(training_data)\n",
    "\n",
    "    training_data_size = training_data.shape[0]\n",
    "    aug_counter = 0\n",
    "    while aug_counter < (augmented_multiple - 1):\n",
    "        iterator = data_augmented.flow(training_data, training_labels, batch_size=training_data_size)\n",
    "        # iterator = data_augmented.flow(training_data, training_labels, batch_size=batch_size)\n",
    "        augmented_data = iterator.next()\n",
    "        for data in augmented_data[0]:\n",
    "            complete_training_data_set.append(data)\n",
    "        for label in augmented_data[1]:\n",
    "            complete_training_labels_set.append(label)\n",
    "        aug_counter += 1\n",
    "\n",
    "    print(\"Size of All Training Data: \" + str(len(complete_training_data_set)))\n",
    "    print(\"Size of All Training Labels: \" + str(len(complete_training_labels_set)))\n",
    "\n",
    "    array_training_data = np.array(complete_training_data_set)\n",
    "    array_training_labels = np.array(complete_training_labels_set)\n",
    "\n",
    "    print(\"Shape of complete training data: \" + str(array_training_data.shape))\n",
    "    print(\"Shape of complete training labels: \" + str(array_training_labels.shape))\n",
    "\n",
    "    return np.array(complete_training_data_set), np.array(complete_training_labels_set)\n",
    "\n",
    "\n",
    "def savePredictedLenses(des_names_array, predicted_class_probabilities, predicted_lenses_filepath, text_file_path):\n",
    "    \"\"\"\n",
    "    This saves the names of the predicted lenses in the respective textfiles.\n",
    "    Args:\n",
    "        des_names_array(array): This is a list of the des names of the sources.\n",
    "        predicted_class_probabilities(list):    This is a list of the probabilities in which lenses are predicted by\n",
    "                                                the algorithm.\n",
    "        predicted_lenses_filepath(string):      This is the string of the predicted lenses filepath, where this needs\n",
    "                                                to be saved in the directory.\n",
    "        text_file_path(string):                 This is the text file path address to which these images are saved.\n",
    "    Saves:\n",
    "        text_file(.txt file):                   This is the text file saved containing the predicted lenses DES names.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(predicted_lenses_filepath):\n",
    "        os.mkdir('%s/' % predicted_lenses_filepath)\n",
    "    text_file = open('%s' % text_file_path, \"a+\")\n",
    "    text_file.write('Predicted Lenses: \\n')\n",
    "    for lens_index in range(len(predicted_class_probabilities)):\n",
    "        if predicted_class_probabilities[lens_index] == 1:\n",
    "            text_file.write(\"%s \\n \" % des_names_array[lens_index])\n",
    "\n",
    "    text_file.write('\\n')\n",
    "    text_file.write('\\n')\n",
    "\n",
    "    text_file.write('No Lenses Predicted: \\n')\n",
    "    for lens_index in range(len(predicted_class_probabilities)):\n",
    "        if predicted_class_probabilities[lens_index] == 0:\n",
    "            text_file.write(\"%s \\n \" % des_names_array[lens_index])\n",
    "    text_file.close()\n",
    "\n",
    "\n",
    "def gettingTrueFalsePositiveNegatives(testing_data, testing_labels, text_file_path,\n",
    "                                      predicted_lenses_filepath):\n",
    "    \"\"\"\n",
    "    This is used to get the True/False Positive and Negative values gained from the CNN confusion matrix.\n",
    "    Args:\n",
    "        testing_data(numpy array):          This is the unseen testing data numpy array.\n",
    "        testing_labels(numpy array):        This is the unseen testing label numpy array.\n",
    "        text_file_path(string):             This is the file path name of the text file in which the confusion\n",
    "                                            matrix is saved.\n",
    "        predicted_lenses_filepath(string):  This is the file path in which the text file is saved.\n",
    "    Saves:\n",
    "        This saves a confusion matrix of the True/False Positive and Negative values.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(predicted_lenses_filepath):\n",
    "        os.mkdir('%s/' % predicted_lenses_filepath)\n",
    "\n",
    "    predicted_data = classifier.predict_classes(testing_data)\n",
    "    true_negative, false_positive, false_negative, true_positive = confusion_matrix(testing_labels,\n",
    "                                                                                    predicted_data.round()).ravel()\n",
    "    matrix = (confusion_matrix(testing_labels, predicted_data.round()))\n",
    "    print(str(matrix) + ' \\n ')\n",
    "    print(\"True Positive: %s \\n\" % true_positive)\n",
    "    print(\"False Negative: %s \\n\" % false_negative)\n",
    "    print(\"False Positive: %s \\n\" % false_positive)\n",
    "    print(\"True Negative: %s \\n\" % true_negative)\n",
    "\n",
    "    text_file = open('%s' % text_file_path, \"w\")\n",
    "    text_file.write('Predicted vs True Matrix: \\n')\n",
    "    text_file.write(str(matrix) + \" \\n \")\n",
    "    text_file.write(\"True Negative: %s \\n\" % str(true_negative))\n",
    "    text_file.write(\"False Positive: %s \\n\" % str(false_positive))\n",
    "    text_file.write(\"False Negative: %s \\n\" % str(false_negative))\n",
    "    text_file.write(\"True Positive: %s \\n\" % str(true_positive))\n",
    "    text_file.write(\"\\n\")\n",
    "    text_file.close()\n",
    "\n",
    "    confusion_matrix_array = [true_negative, false_positive, false_negative, true_positive]\n",
    "    return confusion_matrix_array\n",
    "\n",
    "\n",
    "def gettingKFoldConfusionMatrix(test_data, test_labels, images_47, labels_47, images_84, labels_84, all_unseen_images, all_unseen_labels):\n",
    "    test_confusion_matrix = gettingTrueFalsePositiveNegatives(test_data,\n",
    "                                                              test_labels,\n",
    "                                                              text_file_path='../Results/%s/TrainingTestingResults/KFold_PredictedMatrix.txt' % dt_string,\n",
    "                                                              predicted_lenses_filepath='../Results/%s/TrainingTestingResults' % dt_string)\n",
    "\n",
    "    confusion_matrix_47 = gettingTrueFalsePositiveNegatives(images_47,\n",
    "                                                            labels_47,\n",
    "                                                            text_file_path='../Results/%s/Predicted47/KFold_47_LensesPredicted.txt' % dt_string,\n",
    "                                                            predicted_lenses_filepath='../Results/%s/Predicted47' % dt_string)\n",
    "\n",
    "    confusion_matrix_84 = gettingTrueFalsePositiveNegatives(images_84,\n",
    "                                                            labels_84,\n",
    "                                                            text_file_path='../Results/%s/Predicted84/KFold_84_LensesPredicted.txt' % dt_string,\n",
    "                                                            predicted_lenses_filepath='../Results/%s/Predicted84'% dt_string)\n",
    "\n",
    "    all_confusion_matrix = gettingTrueFalsePositiveNegatives(all_unseen_images,\n",
    "                                                             all_unseen_labels,\n",
    "                                                             text_file_path='../Results/%s/All_Predicted/KFold_All_LensesPredicted.txt' % dt_string,\n",
    "                                                             predicted_lenses_filepath='../Results/%s/All_Predicted' % dt_string)\n",
    "\n",
    "    return test_confusion_matrix, confusion_matrix_47, confusion_matrix_84, all_confusion_matrix\n",
    "\n",
    "\n",
    "def executeKFoldValidation(train_data, train_labels, val_data, val_labels, test_data, test_labels,\n",
    "                           images_47, labels_47, images_84, labels_84, all_unseen_images, all_unseen_labels):\n",
    "    \"\"\"\n",
    "    This does the k fold cross validation which is tested against the unseen testing and known lenses.\n",
    "    Args:\n",
    "        train_data(numpy arrays):           This is the numpy array of the training data.\n",
    "        train_labels(numpy arrays):         This is the numpy array of the training labels.\n",
    "        val_data(numpy arrays):             This is the numpy array of the validation data.\n",
    "        val_labels(numpy arrays):           This is the numpy array of the validation labels.\n",
    "        testing_data(numpy array):          This is the numpy array of the unseen testing data.\n",
    "        testing_labels(numpy array):        This is the numpy array of the unseen testing label.\n",
    "        images_47(numpy array):             This is the numpy array of the unseen DES images data.\n",
    "        labels_47(numpy array):             This is the numpy array of the unseen DES images labels.\n",
    "        images_84(numpy array):             This is the numpy array of the unseen Jacobs images data.\n",
    "        labels_84(numpy array):             This is the numpy array of the unseen Jacobs images labels.\n",
    "        all_unseen_images(numpy array):     This is the numpy array of the unseen DES + Jacobs images data.\n",
    "        all_unseen_labels(numpy array):     This is the numpy array of the unseen DES + Jacobs images labels.\n",
    "\n",
    "    Saves:\n",
    "        This saves the scores, mean and std. of the unseen data that is evaluated in the k fold cross validation.\n",
    "    \"\"\"\n",
    "    if run_k_fold_validation:\n",
    "        print(\"In executingKFoldValidation\")\n",
    "\n",
    "        # this is doing it manually:\n",
    "        kfold = StratifiedKFold(n_splits=k_fold_num, shuffle=True)\n",
    "\n",
    "        test_scores_list = []\n",
    "        unseen_47_scores_list = []\n",
    "        unseen_84_scores_list = []\n",
    "        all_unseen_scores_list = []\n",
    "\n",
    "        test_matrix_list = []\n",
    "        matrix_47_list = []\n",
    "        matrix_84_list = []\n",
    "        all_matrix_list = []\n",
    "\n",
    "        \n",
    "        kf_counter = 0\n",
    "\n",
    "        for train, test in kfold.split(train_data, train_labels):\n",
    "            K.clear_session()\n",
    "            kf_counter += 1\n",
    "            print('KFold #:',kf_counter)\n",
    "\n",
    "            # make the model\n",
    "            model = buildClassifier()\n",
    "            # fit the model\n",
    "            model.fit(train_data[train],\n",
    "                      train_labels[train],\n",
    "                      epochs=epochs,\n",
    "                      validation_data=(val_data, val_labels),\n",
    "                      batch_size=batch_size\n",
    "                      )\n",
    "\n",
    "            unseen_47_scores = model.evaluate(images_47, labels_47, batch_size=batch_size)\n",
    "            unseen_47_scores_list.append(unseen_47_scores[1] * 100)\n",
    "            unseen_84_scores = model.evaluate(images_84, labels_84, batch_size=batch_size)\n",
    "            unseen_84_scores_list.append(unseen_84_scores[1] * 100)\n",
    "            test_scores = model.evaluate(test_data, test_labels, batch_size=batch_size)\n",
    "            test_scores_list.append(test_scores[1] * 100)\n",
    "            all_unseen_score = model.evaluate(all_unseen_images, all_unseen_labels, batch_size=batch_size)\n",
    "            all_unseen_scores_list.append(all_unseen_score[1] * 100)\n",
    "\n",
    "            # show confusion matrix\n",
    "            test_confusion_matrix, confusion_matrix_47, confusion_matrix_84, all_confusion_matrix = gettingKFoldConfusionMatrix(test_data, test_labels, images_47, labels_47, images_84, labels_84, all_unseen_images, all_unseen_labels)\n",
    "            test_matrix_list.append(test_confusion_matrix)\n",
    "            matrix_47_list.append(confusion_matrix_47)\n",
    "            matrix_84_list.append(confusion_matrix_84)\n",
    "            all_matrix_list.append(all_confusion_matrix)\n",
    "\n",
    "        test_scores_mean = np.mean(test_scores_list)\n",
    "        test_scores_std = np.std(test_scores_list)\n",
    "        unseen_47_mean = np.mean(unseen_47_scores_list)\n",
    "        unseen_47_std = np.std(unseen_47_scores_list)\n",
    "        unseen_84_mean = np.mean(unseen_84_scores_list)\n",
    "        unseen_84_std = np.std(unseen_84_scores_list)\n",
    "        all_unseen_mean = np.mean(all_unseen_scores_list)\n",
    "        all_unseen_std = np.std(all_unseen_scores_list)\n",
    "\n",
    "        print(\"Test Scores: \" + str(test_scores_list))\n",
    "        print(\"Test Scores Mean: \" + str(test_scores_mean))\n",
    "        print(\"Test Scores Std: \" + str(test_scores_std))\n",
    "        print(\"Unseen 47 Scores: \" + str(unseen_47_scores_list))\n",
    "        print(\"Unseen 47 Scores Mean: \" + str(unseen_47_mean))\n",
    "        print(\"Unseen 47 Scores Std: \" + str(unseen_47_std))\n",
    "        print(\"Unseen 84 Scores: \" + str(unseen_84_scores_list))\n",
    "        print(\"Unseen 84 Scores Mean: \" + str(unseen_84_mean))\n",
    "        print(\"Unseen 84 Scores Std: \" + str(unseen_84_std))\n",
    "        print(\"All Unseen Scores: \" + str(all_unseen_scores_list))\n",
    "        print(\"All Unseen Scores Mean: \" + str(all_unseen_mean))\n",
    "        print(\"All Unseen Scores Std: \" + str(all_unseen_std))\n",
    "        print(\"Test Confusion Matrices: \" +str(test_matrix_list))\n",
    "        print(\"47 Confusion Matrices: \" +str(matrix_47_list))\n",
    "        print(\"84 Confusion Matrices: \"+str(matrix_84_list))\n",
    "        print(\"All Confusion Matrices: \"+str(all_matrix_list))\n",
    "\n",
    "        excel_headers.append(\"Test Scores Mean\")\n",
    "        excel_dictionary.append(test_scores_mean)\n",
    "        excel_headers.append(\"Test Scores Std\")\n",
    "        excel_dictionary.append(test_scores_std)\n",
    "        excel_headers.append(\"Unseen 47 Scores Mean\")\n",
    "        excel_dictionary.append(unseen_47_mean)\n",
    "        excel_headers.append(\"Unseen 47 Scores Std\")\n",
    "        excel_dictionary.append(unseen_47_std)\n",
    "        excel_headers.append(\"Unseen 84 Scores Mean\")\n",
    "        excel_dictionary.append(unseen_84_mean)\n",
    "        excel_headers.append(\"Unseen 84 Scores Std\")\n",
    "        excel_dictionary.append(unseen_84_std)\n",
    "        excel_headers.append(\"All Unseen Scores Mean\")\n",
    "        excel_dictionary.append(all_unseen_mean)\n",
    "        excel_headers.append(\"All Unseen Scores Std\")\n",
    "        excel_dictionary.append(all_unseen_std)\n",
    "\n",
    "        plt.plot(test_scores_list, color='red', label='Testing Scores')\n",
    "        plt.plot(unseen_47_scores_list, color='blue', label='Unseen 47 Scores')\n",
    "        plt.plot(unseen_84_scores_list, color='black', label='Unseen 84 Scores')\n",
    "        plt.plot(all_unseen_scores_list, color='green', label='Unseen Scores')\n",
    "        plt.xlabel('Folds')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Positive Shape: (2000, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "# Get positive training data\n",
    "train_pos = getPositiveImages('Training/PositiveAll', max_num, input_shape=input_shape)\n",
    "print(\"Train Positive Shape: \" + str(train_pos.shape))\n",
    "excel_headers.append(\"Train_Positive_Shape\")\n",
    "excel_dictionary.append(train_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Negative Shape: (1998, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "# Get negative training data\n",
    "train_neg = getNegativeImages('Training/Negative', max_num, input_shape=input_shape)\n",
    "print(\"Train Negative Shape: \" + str(train_neg.shape))\n",
    "excel_headers.append(\"Train_Negative_Shape\")\n",
    "excel_dictionary.append(train_neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Complete Training Data: 3998\n",
      "Complete Training Label: 3998\n",
      "Size of All Training Data: 7996\n",
      "Size of All Training Labels: 7996\n",
      "Shape of complete training data: (7996, 100, 100, 3)\n",
      "Shape of complete training labels: (7996,)\n"
     ]
    }
   ],
   "source": [
    "all_training_data, all_training_labels, _ = makeImageSet(train_pos, train_neg, shuffle_needed=use_shuffle)\n",
    "if use_augmented_data:\n",
    "    all_training_data, all_training_labels = createAugmentedData(all_training_data, all_training_labels)\n",
    "\n",
    "training_data, val_data, training_labels, val_labels = train_test_split(all_training_data,\n",
    "                                                                        all_training_labels,\n",
    "                                                                        test_size=validation_split,\n",
    "                                                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_headers.append(\"All_Training_Data_Shape\")\n",
    "excel_dictionary.append(all_training_labels.shape)\n",
    "excel_headers.append(\"All_Training_Labels_Shape\")\n",
    "excel_dictionary.append(all_training_labels.shape)\n",
    "excel_headers.append(\"Training_Data_Shape\")\n",
    "excel_dictionary.append(training_data.shape)\n",
    "excel_headers.append(\"Validation_Data_Shape\")\n",
    "excel_dictionary.append(val_data.shape)\n",
    "excel_headers.append(\"Training_Labels_Shape\")\n",
    "excel_dictionary.append(training_labels.shape)\n",
    "excel_headers.append(\"Validation_Labels_Shape\")\n",
    "excel_dictionary.append(val_labels.shape)\n",
    "excel_headers.append(\"Validation_Split\")\n",
    "excel_dictionary.append(validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history, classifier = usingCnnModel(training_data,\n",
    "                                    training_labels,\n",
    "                                    val_data,\n",
    "                                    val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_weights('best_weights.hdf5')\n",
    "classifier.save_weights('galaxies_cnn.h5')\n",
    "\n",
    "excel_headers.append(\"Epochs\")\n",
    "excel_dictionary.append(epochs)\n",
    "excel_headers.append(\"Batch_size\")\n",
    "excel_dictionary.append(batch_size)\n",
    "\n",
    "# Plot run metrics\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "number_of_completed_epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Accuracies\n",
    "train_val_accuracy_figure = plt.figure()\n",
    "plt.plot(number_of_completed_epochs, acc, label='Training acc')\n",
    "plt.plot(number_of_completed_epochs, val_acc, label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "# plt.show()\n",
    "train_val_accuracy_figure.savefig('../Results/%s/TrainingValidationAccuracy.png' % dt_string)\n",
    "plt.close()\n",
    "\n",
    "# Losses\n",
    "train_val_loss_figure = plt.figure()\n",
    "plt.plot(number_of_completed_epochs, loss, label='Training loss')\n",
    "plt.plot(number_of_completed_epochs, val_loss, label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "# plt.show()\n",
    "train_val_loss_figure.savefig('../Results/%s/TrainingValidationLoss.png' % dt_string)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make positive and negative directory\n",
    "if not os.path.exists('../Results/%s/PositiveResults/' % dt_string):\n",
    "    os.mkdir('../Results/%s/PositiveResults/' % dt_string)\n",
    "\n",
    "if not os.path.exists('../Results/%s/NegativeResults/' % dt_string):\n",
    "    os.mkdir('../Results/%s/NegativeResults/' % dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original positive image\n",
    "img_positive_tensor = getPositiveImages('Training/PositiveAll', 1, input_shape=input_shape)\n",
    "positive_train_figure = plt.figure()\n",
    "plt.imshow(img_positive_tensor[0])\n",
    "# plt.show()\n",
    "print(img_positive_tensor.shape)\n",
    "positive_train_figure.savefig('../Results/%s/PositiveResults/PositiveTrainingFigure.png' % dt_string)\n",
    "plt.close()\n",
    "\n",
    "# Visualise Activations of positive image\n",
    "visualiseActivations(img_positive_tensor, base_dir='../Results/%s/PositiveResults/' % dt_string)\n",
    "\n",
    "# Plot original negative image\n",
    "img_negative_tensor = getNegativeImages('Training/Negative', 1, input_shape=input_shape)\n",
    "negative_train_figure = plt.figure()\n",
    "plt.imshow(img_negative_tensor[0])\n",
    "# plt.show()\n",
    "print(img_negative_tensor.shape)\n",
    "negative_train_figure.savefig('../Results/%s/NegativeResults/NegativeTrainingFigure.png' % dt_string)\n",
    "plt.close()\n",
    "\n",
    "# Visualise Activations of negative image\n",
    "visualiseActivations(img_negative_tensor, base_dir='../Results/%s/NegativeResults/' % dt_string)\n",
    "\n",
    "# Classifier evaluation\n",
    "test_pos = getPositiveImages('Testing/PositiveAll', max_num_testing, input_shape)\n",
    "test_neg = getNegativeImages('Testing/Negative', max_num_testing, input_shape)\n",
    "testing_data, testing_labels, _ = makeImageSet(test_pos, test_neg, shuffle_needed=True)\n",
    "print(\"Testing Data Shape:  \" + str(testing_data.shape))\n",
    "print(\"Testing Labels Shape: \" + str(testing_labels.shape))\n",
    "print(\"Got Unseen Testing data\")\n",
    "\n",
    "scores = classifier.evaluate(testing_data, testing_labels, batch_size=batch_size)\n",
    "print(\"Test loss: %s\" % scores[0])\n",
    "print(\"Test accuracy: %s\" % scores[1])\n",
    "\n",
    "excel_headers.append(\"Test_Loss\")\n",
    "excel_dictionary.append(scores[0])\n",
    "excel_headers.append(\"Test_Accuracy\")\n",
    "excel_dictionary.append(scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gettingTrueFalsePositiveNegatives(testing_data,\n",
    "                                  testing_labels,\n",
    "                                  text_file_path='../Results/%s/TrainingTestingResults/ActualPredictedMatrix.txt' % dt_string,\n",
    "                                  predicted_lenses_filepath='../Results/%s/TrainingTestingResults' % dt_string)\n",
    "\n",
    "# Evaluate known 47 with negative 47\n",
    "known_47_images = getUnseenData('UnseenData/Known47', max_num_prediction, input_shape=input_shape)\n",
    "negative_47_images = getUnseenData('UnseenData/Negative47', 47, input_shape=input_shape)\n",
    "images_47, labels_47, des_47_names = makeImageSet(list(known_47_images.values()),\n",
    "                                                  list(negative_47_images.values()),\n",
    "                                                  list(known_47_images.keys()),\n",
    "                                                  list(negative_47_images.keys()),\n",
    "                                                  shuffle_needed=True)\n",
    "print(\"47 Data Shape:  \" + str(images_47.shape))\n",
    "print(\"47 Labels Shape: \" + str(labels_47.shape))\n",
    "print(\"Got Unseen 47 data\")\n",
    "\n",
    "predicted_class_probabilities_47 = classifier.predict_classes(images_47, batch_size=batch_size)\n",
    "lens_predicted_count_47 = np.count_nonzero(predicted_class_probabilities_47 == 1)\n",
    "non_lens_predicted_count_47 = np.count_nonzero(predicted_class_probabilities_47 == 0)\n",
    "print(\"%s/47 known images predicted\" % lens_predicted_count_47)\n",
    "print(\"%s/47 non lensed images predicted\" % non_lens_predicted_count_47)\n",
    "\n",
    "gettingTrueFalsePositiveNegatives(images_47,\n",
    "                                  labels_47,\n",
    "                                  text_file_path='../Results/%s/Predicted47/47_LensesPredicted.txt' % dt_string,\n",
    "                                  predicted_lenses_filepath='../Results/%s/Predicted47' % dt_string)\n",
    "\n",
    "savePredictedLenses(des_47_names,\n",
    "                    predicted_class_probabilities_47,\n",
    "                    predicted_lenses_filepath='../Results/%s/Predicted47' % dt_string,\n",
    "                    text_file_path='../Results/%s/Predicted47/47_LensesPredicted.txt' % dt_string)\n",
    "\n",
    "excel_headers.append(\"Predicted_Lens_47\")\n",
    "excel_dictionary.append(lens_predicted_count_47)\n",
    "excel_headers.append(\"Predicted_No_Lens_47\")\n",
    "excel_dictionary.append(non_lens_predicted_count_47)\n",
    "\n",
    "# Evaluate known 84 with negative 84\n",
    "known_84_images = getUnseenData('UnseenData/Known84', max_num_prediction, input_shape=input_shape)\n",
    "negative_84_images = getUnseenData('UnseenData/Negative84', 84, input_shape=input_shape)\n",
    "images_84, labels_84, des_84_names = makeImageSet(list(known_84_images.values()),\n",
    "                                                  list(negative_84_images.values()),\n",
    "                                                  list(known_84_images.keys()),\n",
    "                                                  list(negative_84_images.keys()),\n",
    "                                                  shuffle_needed=True)\n",
    "print(\"84 Data Shape:  \" + str(images_84.shape))\n",
    "print(\"84 Labels Shape: \" + str(labels_84.shape))\n",
    "print(\"Got Unseen 84 data\")\n",
    "\n",
    "predicted_class_probabilities_84 = classifier.predict_classes(images_84, batch_size=batch_size)\n",
    "lens_predicted_count_84 = np.count_nonzero(predicted_class_probabilities_84 == 1)\n",
    "non_lens_predicted_count_84 = np.count_nonzero(predicted_class_probabilities_84 == 0)\n",
    "print(\"%s/84 known images predicted\" % lens_predicted_count_84)\n",
    "print(\"%s/84 non lensed images predicted\" % non_lens_predicted_count_84)\n",
    "\n",
    "gettingTrueFalsePositiveNegatives(images_84,\n",
    "                                  labels_84,\n",
    "                                  text_file_path='../Results/%s/Predicted84/84_LensesPredicted.txt' % dt_string,\n",
    "                                  predicted_lenses_filepath='../Results/%s/Predicted84' % dt_string)\n",
    "\n",
    "savePredictedLenses(des_84_names,\n",
    "                    predicted_class_probabilities_84,\n",
    "                    predicted_lenses_filepath='../Results/%s/Predicted84' % dt_string,\n",
    "                    text_file_path='../Results/%s/Predicted84/84_LensesPredicted.txt' % dt_string)\n",
    "\n",
    "excel_headers.append(\"Predicted_Lens_84\")\n",
    "excel_dictionary.append(lens_predicted_count_84)\n",
    "excel_headers.append(\"Predicted_No_Lens_84\")\n",
    "excel_dictionary.append(non_lens_predicted_count_84)\n",
    "\n",
    "all_unseen_images = np.concatenate((images_47, images_84))\n",
    "all_unseen_labels = np.concatenate((labels_47, labels_84))\n",
    "all_des_names = np.concatenate((des_47_names, des_84_names))\n",
    "print(\"All Data Shape: \" + str(all_unseen_images.shape))\n",
    "print(\"All Data Labels: \" + str(all_unseen_labels.shape))\n",
    "\n",
    "all_predicted_class_probabilities = classifier.predict_classes(all_unseen_images, batch_size=batch_size)\n",
    "all_lens_predicted_count = np.count_nonzero(all_predicted_class_probabilities == 1)\n",
    "all_non_lens_predicted_count = np.count_nonzero(all_predicted_class_probabilities == 0)\n",
    "print(\"%s/131 known images predicted\" % all_lens_predicted_count)\n",
    "print(\"%s/131 non lensed images predicted\" % all_non_lens_predicted_count)\n",
    "\n",
    "gettingTrueFalsePositiveNegatives(all_unseen_images,\n",
    "                                  all_unseen_labels,\n",
    "                                  text_file_path='../Results/%s/PredictedAll/All_LensesPredicted.txt' % dt_string,\n",
    "                                  predicted_lenses_filepath='../Results/%s/PredictedAll' % dt_string)\n",
    "\n",
    "savePredictedLenses(all_des_names,\n",
    "                    all_predicted_class_probabilities,\n",
    "                    predicted_lenses_filepath='../Results/%s/PredictedAll' % dt_string,\n",
    "                    text_file_path='../Results/%s/Predicted84/All_LensesPredicted.txt' % dt_string)\n",
    "\n",
    "excel_headers.append(\"Predicted_Lens_All\")\n",
    "excel_dictionary.append(all_lens_predicted_count)\n",
    "excel_headers.append(\"Predicted_No_Lens_All\")\n",
    "excel_dictionary.append(all_non_lens_predicted_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K fold for training data\n",
    "executeKFoldValidation(training_data, training_labels, val_data, val_labels, testing_data, testing_labels, images_47, labels_47, images_84, labels_84, all_unseen_images, all_unseen_labels)\n",
    "\n",
    "print(\"Test loss of normal CNN: %s\" % scores[0])\n",
    "print(\"Test accuracy of normal CNN: %s\" % scores[1])\n",
    "\n",
    "# add row to excel table\n",
    "if makeNewCSVFile:\n",
    "    createExcelSheet('../Results/new_kerasCNN_Results.csv', excel_headers)\n",
    "    writeToFile('../Results/new_kerasCNN_Results.csv', excel_dictionary)\n",
    "else:\n",
    "    writeToFile('../Results/new_kerasCNN_Results.csv', excel_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'../Results/%s/PredictedAll' % dt_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_47.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_47.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_84.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_84.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unseen_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unseen_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "94+168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bita95efc697fa945fe88e19ee2c671c4e4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}